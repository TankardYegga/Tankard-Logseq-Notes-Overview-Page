- DONE 阅读intro，先总结下可以拆成几段 [[PaperWriting]]
  collapsed:: true
  :LOGBOOK:
  CLOCK: [2022-10-29 Sat 14:11:33]
  CLOCK: [2022-10-29 Sat 14:11:37]--[2022-10-31 Mon 10:49:43] =>  44:38:06
  :END:
	- In this work, we aim to improve the prediction of the pathological state of MCs by adapting the fusion scheme of vanilla weighted averaging. Firstly, we execute weighted fusion on the level of the individual feature instead of the whole model. In the original method, a weight corresponds to all the individual features (i.e. the feature vector) extracted by a single model. Obviously, this is unfair because different features have different degrees of interpretability and relevance, which can be learned from radiomics.  Therefore, we propose the strategy of multi-group weighted fusion. We treat all the individual features in the same position of feature vectors as in the same group and fuse the features only within each group. Secondly, the within-group weights must be dynamically calculated and data-adaptive. This is required by the large variance of calcification points among images and the richness of model combinations. We address the problem by designing a novel similarity-decoding-based weighted fusion mechanism, abbreviated as SDWFM. A learnable latent vector L is introduced to denote the most essential features related to classification. Individual features within every group are linearly transformed into vectors and then these vectors are used to calculate the inner product with L respectively. Undoubtedly,  the inner product operation can decode the similarity between L and each individual feature, and the normalized results represent the relative importance of different features.
	-
	- In our implementation, we choose two models, resnet18 and densenet121, as the fusion object. We limit the number of models here to 2 to reduce the complexity. For the model selection, resnet18 and densenet121 can theoretically capture features that complement each other, as the former is good at extracting shallow features and reusing them, while the latter is good at extracting deep features and discovering new ones[]. Additionally, resnet18 and densenet121 are separately the one with the fewest parameters in the resnet and densenet network architectures, which is suitable for our small in-house dataset.
	-
	- This step scales all the individual features into one-dimensional vectors using the same linear matrix. The goal is to provide more information while maintaining the relationship of individual features.
	-
	- An overview of the proposed model is depicted in Fig. 3. The proposed mainly contains three subnets, namely,  the resnet18 branch (green path),  the densenet121 branch (blue path), and the model fusion branch (orange path).
	-
	- The two branches of resnet18 and densenet121 are used as backbones for feature extraction. Notably, we adjust their structures from a conventional single Fc (fully connected layer) into two Fcs $fc_1$ and $fc_2$ (see Fig.5). The reason is that the number of output neurons of the Fc (i.e. the number of categories N) is insufficient for effective info fusion. Specifically, the $fc_1$ enhances info density by increasing the number of output neurons, the output of which is where the two models fuse; the $fc_2$ keeps the number of output neurons the same as the original Fc (i.e. N), adding auxiliary outputs for classification. The two branches generate $c_1$ -lengthy vectors $m$ and $n$  separately right after corresponding $fc_1$ .
	-
	- The fusion branch consists of a model fusion block and a final classification head.  The former fuses the $m$ and $n$ into one vector $fused\_v$ through the multi-group similarity-decoding-based weighted fusion mechanism (abbreviated as MG-SDWFM). The latter passes $fused\_v$ through the last $fc$ to obtain the final prediction.
	-
	- The whole model is trained in an end-to-end manner using the multi-output weighted loss function.
	-
	-
	- 毫无疑问，权重反映的是特征与分类的相关性大小。假设有一个长度为d的向量L，其包含了理想上最有用的特征。那么L与向量化后的单个特征的内积，能够粗略地作为相关性的指标。因为内积越大，代表L和该特征的相似性越高，该特征的相关性也就越大。为了保持一致性，L在所有单个特征上进行共享。在同一组内两个内积计算结果进行归一化即可得到权重。这个过程可以形式化为：
	- 在上述过程中，我们解码出了L与每个特征的相似性，继而得到了同一组内不同特征的相对重要性。单独的L本身是难以学习的，但是共享的L是可以学习的。这个过程可以被称作是。
	- Clearly, weights reflect the magnitude of the features' relevance to the classification. Assume there is a vector $L$ of length $d$ that ideally contains the most useful features. Then the inner product of L and the vectorized individual feature can be used approximatively as a measure of relevance. This is because a larger inner product indicates a greater similarity between L and that feature, as well as a greater relevance for that feature. To maintain consistency, L is shared across all individual features. Normalizing two inner product calculations performed within the same group yields the weights. In ith group, for vectorized individual feature $s_i$ and $t_i$ , the process can be formulated as:
	- In the above process, we decode the similarity of $L$ with each feature and subsequently obtain the relative importance of different features within the same group. The L alone is difficult to learn, but the shared L is learnable. We call this procedure the similarity-decoding-based weighted fusion mechanism, dubbed SDWFM.
	-
	-
	- The proposed fusion scheme has two core ideas. The first one is the concept of "multi-group". We take all the individual features at each position on the feature vectors as a group. Thus, the number of groups is equal to the length of the feature vector. The second one is the data-adaptive implementation of weights calculation and weighted summation within every group of individual features.
	-
	- The number of individual features within each group is equal to the total number of models, which in our case is 2. In the ith group ( i.e. the ith position ), the two individual features $m_i$ and $n_i$ don't necessarily have to be semantically related, because the weighted summation operation can compress these two distinctive features into a more valid and compact one. The effect of compression is closely related to the setting of weights. For different models and input images, weights are supposed to be different.
	-
	-
	- In this paper, we improve vanilla weighted averaging in terms of the fusion scope and weight calculation, making it data-adaptive. We adopt a novel multi-group weighted fusion scheme, narrowing down the fusion scope from the entire model to each single feature corresponding to each position of the feature vector. Within every group of single features, a share latent embedding is utilized to decode their similarities and then obtain their relative importance in the form of weights. Therefore, we call the proposed as multi-group similarity-decoding-based weighted fusion mechanism, denoted as MG-SDWFM.
	-
	- We also add an auxiliary output for each model to provide supervision, and devise a multi-output loss to help the model train in one pass.
	-
	- In this paper, a multi-group similarity-decoding-based weighted fusion mechanism, dubbed MG-SDWFM, is proposed, which is an adaption of vanilla weighted averaging. We divide all individual features into multiple groups according to the position and execute fusion only within the group. In the fusion process, a shared latent embedding $L$ is introduced to represent the most essential features for classification. With the help of the inner product, the similarity between $L$ and each vectorized feature is calculated and the relative importance of different features is then obtained in the form of weights. Additionally, we add an auxiliary output for each model to provide supervision and devise a multi-output loss to help the model train.
- :LOGBOOK:
  CLOCK: [2022-10-29 Sat 14:14:31]
  :END: