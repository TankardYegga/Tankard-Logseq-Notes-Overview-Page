title:: mit/6.824

- Introduction:
  collapsed:: true
	- 分布式系统是什么呢？
	  collapsed:: true
		- multiple networked cooperating computers
	- 主要使用场景，可以举例吗？
	  collapsed:: true
		- ![image.png](../assets/image_1687421174924_0.png)
		- 协调完成服务器、客户端和数据中心这三者的工作
		- 比如说ZOOM应用
	- 为什么要使用分布式系统？或者说，为什么分布式系统会变得流行起来？
	  collapsed:: true
		- Connect [[$red]]==physical==ly separated machines => allow data sharing
			- 当我和你的两台机器连接到同一台电脑时，我们可以分享数据、共享文件、共享屏幕、共享计算基础设施，即能够允许一系列的协作可能性
		- Increase capacity through [[$red]]==parallelism==
			- MapReduce是一个重要的例子
			- 在同一个时间段内有很多的Zoom Session在进行
		- [[$red]]==Tolerate faults==
			- 当一个机器或者部分损坏时，不会影响到另外一个机器或者部分，比如存储可以始终分发
			- 容错能力保证了整个系统的可用性
		- Achieve [[$red]]==security== by physical separation / isolation
			- 比如如果你有一个管理顾客密码的服务，所以通常会把它对外的接口设计得非常狭窄
		- 四个重要原因中第二个和第三个是最重要的
	- 分布式系统的Historical Context大概是什么样的？
	  collapsed:: true
		- 1980s年代的local area network
			- 比如MIT的校园网络、连接像Athena集群一样的工作站 (Athena Clusters (亚瑟纳集群) 是谷歌开发的一种分布式计算框架，主要用于数据分析和大规模数据处理)
			- AFS（Andrew File System）是一种分布式文件系统也是在那时发明出来
			- 那时候的主要应用只有DNS和Email系统
		- The rise of Data Centers along with big websites (1990s or early 1990s)
			- web search: 大量的数据进行indexing不能放在一个电脑上
			- shopping: 有大量的用户
			- MapReduce的论文也是在这个时期出来的
		- Cloud Computing (mid late 2000s)
			- 这个过程随着云计算的出现而加速了
			- 将数据和计算移动到数据中心
		- Current state: active
	- 分布式系统的Challenge有哪些？
	  collapsed:: true
		- 两个主要的挑战为：
			- many concurrent parts
				- 并行的计算机数目太多，导致难以推断和理解整个运行的过程
			- must deal with partial failure
		- 这两个挑战导致的问题是：
			- 高并发度和部分出错导致系统的复杂度越来越高，比如系统的一个部分可能认为系统的另外一个部分故障了，但是实际可能不是的，实际可能的是有一个网络划分(network partition)，被划分的系统两边分别保持计算，分别与一部分client进行交互，甚至两边交互的这部分client是完全一样的，因为client可以与系统两边都发生交互，而系统的这两边并不发生交互，这个问题被称作Split Brain Syndrome（大脑分裂综合征）,  这使得design distributed system和protocol statistical system变得复杂；
		- 最后一个挑战是：
			- tricky to realize the performance benefits that in principle are possible with distributed system
				- 也就是在给定数量的机器下来实现high throughput和throughput scaling并没有那么简单直接
	- 为什么要学习这门课？
	  collapsed:: true
		- ![image.png](../assets/image_1687424771020_0.png)
		- hands-on指的是特别的编程体验
	- 怎么学习这门课？
	  collapsed:: true
		- ![image.png](../assets/image_1687425363498_0.png)
		- ![image.png](../assets/image_1687429390407_0.png)
			- 网络系统和分布式系统存在交叉，关于通信的知识主要在MIT6.829中学习到
			- 存储指的是key value存储系统 或者  文件系统
		- ![image.png](../assets/image_1687430828856_0.png)
			- 可恢复性指的是直接重启故障的机器后继续保持可用性，而不是修复系统，因为修复系统必须要将一台一台的机器全部关闭最终导致系统服务不可用
			- 可用性的关键技术是：replication
			- 可恢复性的关键技术是：logging or transactions (durable storage)
			- 一致性希望分布式系统的性能 和 sequential machine尽可能保持一致
			- 不一致性有很多不同的contract，比如最终一致性，关键在于对于Performance的要求不同
			- Performance和Fault tolerance和Consistency是相互矛盾的，为了实现一致性，则需要不同机器之间进行通信，这可能会降低性能；而为了实现容错，则不得不把数据从一台机器复制到其他台机器，而如果把数据写入可持久存储的话，其开销也是非常昂贵的，这会消耗性能。所以，通常会牺牲部分一致性或者部分容错性来提升性能
			- Performance实际包含两个方面：
				- 一个是Throughput, 这个可以通过增加更多的机器来实现
				- 另外一个是low latency,  比如disk不能百分百处理等导致处理用户请求需要很久，
			- 第四个topic是Implementation, 主要是concurrency和RPC
			-
			-
	- MapReduce的历史context是什么？
	  collapsed:: true
		- ![image.png](../assets/image_1687432251142_0.png)
		- Mapeduce方法包含两个部分：
			- 一是map方法和reduce方法，都是functional or stateless function，方法内都是sequential code
			  collapsed:: true
				- Functional function 和 Stateless function 都是程序中的两种函数类型，具体含义如下：
				- Functional function 是指在程序中定义时只参考了输入和输出之间的关系，它不会改变程序的状态或任何外部变量，也没有任何副作用。这种函数通常被称为纯函数，因为它们只关注给定的输入及其返回的输出。由于纯函数不会对程序的状态造成任何影响，因此它们通常易于调试和测试，并可以提高程序的可读性和可维护性。
				- Stateless function 是指在程序中定义时不保留任何状态信息，只根据其输入来计算输出。它们不依赖于任何外部环境或变量，并且不会对程序的状态造成任何影响。这种函数通常用于函数式编程，它们的目标是简单、干净且易于测试的代码。
				- 总之，无论是 Functional function 还是 Stateless function，它们都面向纯粹的函数计算，不依赖外部状态，而是根据输入计算输出。唯一的区别在于前者特别强调不会产生副作用，后者不会保留任何状态信息。
			- 二是处理distributedness，需要解决load balancing、运行缓慢的机器、crash的机器等，使得写map和reduce方法的程序员不用关心这些
		- mapreduce并不是general purposed的？
			- 对，因为需要自己根据case来写对应的map和reduce方法
			- 比如你要写一个key value的服务，将不能使用MapReduce这个库，因为这个库假定了一个特定的计算模型，我们要应用程序必须要fit in这个计算模型，其适用于做一些网页的大数据分析，需要大量计算来处理这些大量的数据，并且基于这些数据来计算值
			-
	- 关于MapReduce的一些问题？
		- 同样的一个Map Task有可能执行两次吗？同样的一个Reduce Task也可能执行两次吗？
		- Map Reduce当中的master可以fail吗？为什么？
		- 一个Map Worker对应的机器上是只执行一个map function还是说可以执行多个map function？
		- 当一个Map Worker对应的机器crash了，要怎么做呢？
- RPCs and threads:
  collapsed:: true
	- 为什么使用Go而不是用C++？
		- C++处理垃圾回收非常麻烦，而多线程应用中需要判断某个对象什么时候不再被任何一个线程使用了
		- C++的语法过于复杂
	- 为什么需要使用线程？
		- I/O  Concurrency
		- Parallelism
			- 充分利用一台机器上的所有核/CPU
		- Convenience
			- 一些程序会需要在后台运行，周期性地运行然后休眠
			- the overhead is worthy ?
				- yes
	- asynchronous program和vent driving program的区别？
	  collapsed:: true
		- Asynchronous programming and event-driven programming are both ways of handling concurrency in software development, but they are not the same thing.
		- Asynchronous programming is a programming paradigm where tasks are executed concurrently without blocking the main thread. This means that a program can execute multiple tasks at the same time, without waiting for one task to complete before starting the next one. Asynchronous programming is typically used in situations where a program needs to perform I/O operations, such as reading from a file or making a network request, without blocking the main thread.
		- Event-driven programming, on the other hand, is a programming paradigm where the flow of the program is determined by events that occur, rather than by a sequential order of execution. In an event-driven program, the program waits for events to occur, such as mouse clicks or keyboard presses, and then responds to those events. Event-driven programming is often used in graphical user interfaces, where the program needs to respond to user input in real-time.
		- While there is some overlap between asynchronous programming and event-driven programming, they are not the same thing. Asynchronous programming is a way of handling concurrency, while event-driven programming is a way of structuring the flow of a program based on events.
		- 线程比事件驱动更容易，因为它只需要写直接的流程控制代码：计算、发送信息、等待响应，
			- 而后者它需要把无论什么活动都分割成很多小的片段，这些小的代码片段会不时地被相应的事件驱动循环在某个时刻激活，因此编程来说会更加困难；此外如果在事件驱动编程下如果你想要获得并发，将无法获得CPU的并行，比如你要写一个保持32核都忙碌的繁忙的服务器，一个单一的循环不是一个自然的方式去利用超过一个核；
			- 通常这种方式的开销会比线程稍微小一点，为什么呢？
				- 因为当多线程的数量超过1000时，管理线程的开销就开始变大，包括维持每个线程的状态、决定下次该执行哪一个线程
		- 进程是由操作系统来控制的，与具体使用什么语言无关
	- 当一个context发生切换时，所有的线程都发生了对应的切换吗？
	  collapsed:: true
		- 假如你有一台单核的机器，你在上面运行了多个进程，OS会给不同的进程time slicing来进行来回切换，所以当硬件计时结束时，CPU从一个进程拿走，被放置到另外一个进程之上；context切换时只有操作系统知道的线程会进行切换
		- go cleverly multiplex as many go routines on top of single operating system threads to reduce overhead
			- 这句话的意思是，在单个操作系统线程上巧妙地多路复用多个Go协程，以减少开销。
			- 在Go语言中，协程（goroutine）是一种轻量级的线程，可以在单个线程上同时运行多个协程，而不需要为每个协程分配一个单独的线程。这种设计可以提高程序的并发性能，但也会带来一些开销，例如上下文切换、协程调度等。
			- 因此，这句话的意思是，为了减少这些开销，可以在单个操作系统线程上巧妙地多路复用（multiplex）多个Go协程，即在单个线程上同时运行多个协程，从而减少上下文切换和调度的开销。这种方式可以提高程序的并发性能，同时也可以减少系统资源的使用
		- 这是一个两阶段的调度，先决定哪个大线程去运行，然后go决定哪一个协程在那个进程中运行
		-
	- 线程有哪些缺点？
	  collapsed:: true
		- Race conditions:
		  collapsed:: true
			- 虽然线程之间可以共享内存或者缓存，但是可能会存在很多的bug，比如下面这个共享变量的例子
			  collapsed:: true
				- ![image.png](../assets/image_1687762038539_0.png)
					- 一般来说存储原语是原子的，而INCREMENT原语非常有可能不是原子的
					- 不同线程运行同一段代码的过程，可以被称作RACE，因为第二个线程可能在第一个写入更新后的X之前或者之后就Load X的值了
			- 怎么解决Race Conditions呢？
			  collapsed:: true
				- 有两种解决方法：
					- 一是Avoid Sharing,
						- 这是Go所鼓励的编程风格：鼓励使用channels来communicate变量的值，而不是通过共享内存
					- 二是使用locking使得一组操作序列变成atomic的
						- Go怎么知道哪个变量在上锁，比如是X + Y这种操作？
						  collapsed:: true
							- Go并不知道任何变量和锁的关系，这是由程序员来控制的
						- 锁应该是private的吗？
						  collapsed:: true
							- 数据结构的私有就好像市政规划地图（Zoning Map）内部有一个lock在保护它。这会是一个合理的策略，define a data structure that needs to be locked to have the lock be sort of interior that have each of the data structure methods be responsible for acquiring that lock and the user the data structure may never know：
							  collapsed:: true
								- This sentence means to create a data structure that requires locking, where the lock is an internal mechanism and each method within the data structure is responsible for acquiring the lock before proceeding with its task. The user who interacts with the data structure may not be aware of the existence of the lock as it is handled automatically by the data structure's methods. This approach helps to ensure thread safety and prevent data corruption from multiple threads accessing the same data simultaneously.
							- 唯一的崩溃点是：
								- 如果一个编程人员知道这个数据将从来不会被分享，他们可能会失望他们为了不需要被locked的东西而付出了额外的锁的开销
								- if there's any inter data structure of dependencies，so we have two data structures  each with locks and they may use each other and then there's risks of cycles and deadlocks
									- 通常解决deadlock的方法是requires lifting the locks out of the implementations up in the calling code（需要将锁从实现层面提升到调用代码层面）
							- 隐藏锁是一个good idea，但不总是
						- Go语言中有一个RaceDetector，大多数的lab会鼓励你使用dash race flag来运行go，这不会捕获每个可能的Race，但是它已经能很好地识别races了，所以默认情况下你应该开启race detector enabled来运行go。
		- Coordination：一个线程必须等待另外一个线程完成某些事
		  collapsed:: true
			- Go中的channel不仅用来交流值，也可以同时用来coordinate
			- 通过条件变量
		- DeadLocks
		  collapsed:: true
			- 在Go中最trivial的出现死锁的情况是：
				- 有一个单线程，此时它block了，等待其他的线程从它的channel里面读取数据，但是很明显并没有其他的channel存在
				- Go将会捕捉到这种情况，并且会报出一个razor runtime error
			-
	- Go语言解决并发问题的两个Plan是什么？
	  collapsed:: true
		- ![image.png](../assets/image_1687772074136_0.png)
		-
	- Go中的条件变量可以被看作是什么？
	  collapsed:: true
		- 可以看作是协调进程之间的coordination原语(primitive)，尤其是当使用lock来保护共享状态(shared state) 的时候
		-
		-
	- Web Crawler项目的目标是什么呢？
	  collapsed:: true
		- ![image.png](../assets/image_1687853212490_0.png)
			- 第二个目标一次性获取到URL，是在说correctness的要求
		-
	- Go当中的RPC是如何工作的？
	  collapsed:: true
		- ![image.png](../assets/image_1687879945764_0.png)
			- marshal 和 unmarshal 是序列化和反序列化的过程。序列化是将数据结构或对象转换为字节流的过程，以便将其从一个应用程序发送到另一个应用程序或存储在磁盘上。反序列化是将字节流转换回原始数据结构或对象的过程
			- 序列化和反序列化的方法可能有所不同，但通常采用比较常见的方式，比如 JSON、XML、Protobuf、MessagePack 等
			- 这些stub使得远程过程调用和regular/local procedure call的效果差异不大，并且这些stub通常都是自动生成的
	- RPC的failures有哪些可能的语义呢？
	  collapsed:: true
		- ![image.png](../assets/image_1687914194519_0.png)
			- 至少一次：
				- 如果server fail了，那么client应该做什么呢？此时当client第一次向crashed的server发送请求时，必然得不到响应，所以会time out，然后client就会自动进行重试，直到server的函数调用至少执行了一次
				- 这种至少一次的rpc系统的downside是同一个操作可能会被执行很多次，这并不适用于许多应用
			- 最多一次：
				- 对应的服务端的请求要么执行1次，要么就不执行，不会超过1次
				- 这典型是由filtering duplicates来实现的
				- 服务器端可能同时接受到两个request，或许这时的网络就像一个临时请愿 (temporary petition) 且服务器接受到两个requests，此时的服务器必须进行管理：先检测a recent request, 但是并不会execute两次
			- 刚刚好一次（理想情况下）：
				- 因为这是normal情况下的procedure call会展现的
				- 但这通常非常难去进行管理或者说安排arrange,  要求你不得不维持磁盘上的状态, 这个维持操作是昂贵的。在实践中，只有很少的RPC系统是exactly once的
	- Go语言中的rpc的failures的语义是哪一种呢？
	  collapsed:: true
		- 是at most once
		- 通过tcp channel来发起调用，tcp channel将会保证没有duplicates。应用程序可能会进行重试，但将是应用程序的责任来处理duplication和failed message的问题
		- rpc和pc的区别是通过failure时的goal来体现的
		-
	-
	-
- GFS：
  collapsed:: true
	- 课程计划是什么？
	  collapsed:: true
		- ![image.png](../assets/image_1687937958780_0.png)
		- ![image.png](../assets/image_1687938329759_0.png)
		-
	- 为什么涉及存储系统会很难？
	  collapsed:: true
		- 存储系统有high performance的要求 =》需要有many servers
			- shared data across multiple servers
		- many servers意味着会有 ==> constant faults
		- constant faults => fault tolerance design =》replication
			- 在存储系统通常的方式是replication，也就是将数据在多个disk上进行复制
		- replication =》potential inconsistency
		- strong consistency protocol =》low performance caused by sending messages
			- 不是发送的消息有多大，而是协议的一部分是对可持久存储进行读或者写，而这个操作是开销昂贵的
			-
			-
	- 分布式存储系统中一致性的两大hazard是什么？
	  collapsed:: true
		- 一是一致性问题
		  collapsed:: true
			- 什么是ideal consistency？
			  collapsed:: true
				- 通过一个具体的执行案例，明确合理的值是什么、不合理的值是什么来定义什么是consistency
					- ![image.png](../assets/image_1687940308288_0.png)
						- C3读取的结果可能是1或者2，这取决于C1和C2并发执行的相对顺序
						- C4读取的结果和C3完全保持一致
						-
				-
		- 二是failure问题
		  collapsed:: true
			- 对于复制没有任何约束时，是最坏的复制策略，也可以视作no protocol
				- ![image.png](../assets/image_1687940892468_0.png)
	- GFS的case study主要学习些什么？
	  collapsed:: true
		- ![image.png](../assets/image_1687943468070_0.png)
	- GFS 有哪些特点？
	  collapsed:: true
		- ![image.png](../assets/image_1687944433857_0.png)
- Primary / backup replication:
  collapsed:: true
	- 课程简介是什么？
	  collapsed:: true
		- Minica Summary：
			- Detailed Summary for [Lecture 4: Primary-Backup Replication
			  不公开](https://www.youtube.com/watch?v=gXiDmq1zDq4) by [Monica](https://monica.im)
			  
			  [00:03](https://www.youtube.com/watch?v=gXiDmq1zDq4&t=3.199) This section is an introduction to primary-backup replication and discusses failures that can be tolerated and challenges in backup schemes.
			- There are failures that can be tolerated using primary-backup replication and some that cannot.
			- The main challenges in any backup scheme are discussed.
			- Two dominant approaches to replication are presented: state transfer replication and replicated state machines.
			- The VMware fault-tolerant system is used as a case study to make everything more concrete.
			    
			  [11:36](https://www.youtube.com/watch?v=gXiDmq1zDq4&t=696.959) The lecture discusses two main approaches for primary-backup replication: state transfer and replicated state machine.
			- Failover means the backup takes over in case of failure.
			- State transfer involves the primary transferring state changes to the backup.
			- Replicated state machine involves sending operations to the backup to update its state.
			    
			  [23:13](https://www.youtube.com/watch?v=gXiDmq1zDq4&t=1393.679) The VMFT paper proposes exploiting virtualization to make business applications more fault-tolerant.
			- The paper suggests using fertilization and virtualization to make replication transparent to the application.
			- The replication scheme used by VMFT provides strong consistency and appears to use a single machine.
			- The paper is about a real product that is still in use, although the current version is different from the one described in the paper.
			    
			  [34:51](https://www.youtube.com/watch?v=gXiDmq1zDq4&t=2091.839) In primary-backup replication, the backup becomes the primary after a failure and both sides can communicate in the case of network partitions.
			- Virtual machine monitor sends packets periodically over the logging channel and if it doesn't get any responses from the other side, it assumes there's a problem.
			- If the network partitions, both primary and backup will notice the problem and at some point, decide to promote themselves to primary.
			- Both sides can still communicate in the case of network partitions and they can go back into single mode with one primary.
			    
			  [46:28](https://www.youtube.com/watch?v=gXiDmq1zDq4&t=2788.4) The section explains the design of primary-backup replication and the role of the arbitration server.
			- The primary and backup servers are connected to a storage server.
			- The storage server has two roles: storage for primary and backup, and arbitration server.
			- The arbitration server has a flag that is initially zero and is used for promoting one of the servers to be the single server that serves client requests.
			    
			  [58:07](https://www.youtube.com/watch?v=gXiDmq1zDq4&t=3487.359) Interrupts, including input packets and timer interrupts, must be delivered at the same point in the instruction stream to avoid divergence in the system state.
			- If interrupts are delivered at different points, the state of the system might be different and produce different results.
			- Concurrency can also produce non-determinism, but controlling interrupts can help control this potential source of divergence.
			- The solution in the paper is to only allow uni-processors to avoid the problem of multi-core divergence.
			    
			  [01:09:43](https://www.youtube.com/watch?v=gXiDmq1zDq4&t=4183.6) The virtual machine monitor executes instructions and sticks the value from the message into the red register to ensure the same effect on both primary and backup in primary-backup replication.
			- The virtual machine monitor doesn't execute the instruction at all.
			- The hypervisor will do some work before putting up the virtual machine.
			- The backup and primary should run at the same speed.
			- Communication over the channel will always happen because of periodic interrupts.
			    
			  [01:21:22](https://www.youtube.com/watch?v=gXiDmq1zDq4&t=4882.48) The lecture discusses the performance of primary-backup replication system.
			- The system runs at the level of machine instructions which results in a performance hit.
			- The left table in the paper shows that performance is good when running the backup on the primary.
			- In the receiving case, there is a 30% reduction in performance when running in backup mode.
		- ![image.png](../assets/image_1688467870713_0.png){:height 421, :width 718}
	- replication scheme可以handle和不可以handle的failures分别有哪些呢？
	  collapsed:: true
		- 可以处理的：
		  collapsed:: true
			- ![image.png](../assets/image_1688468275423_0.png)
			- 由计算机的component造成的计算机运行停止；从working到not working的时间非常短；不会产生奇怪的错误结果
			- 案例包括：
				- 电脑的风扇坏了但是电脑overheat会导致电脑自动shut down
				- 踩到了power cord（充电的线缆）造成了爆炸，整个电脑消失
				- 切断网络连接
			- 计算机有时候会把软件上的failures给转化成fail-stop failure
				- 比如某个软件计算数据的checksum，如果发现不正确，可能会stop the computer
				-
			-
		- 不可以处理的：
		  collapsed:: true
			- logic bugs
				- 在primary和replication中软件的逻辑错误将会重复出现
			- configuration errors
			- malicious errors
				- 比如黑客故意发送不正确的数据来spoof（伪装、欺骗）整个system
				-
		- may or may not be handled:
			- 比如earthquake，我们会希望backup能够维持原有的数据
			-
	- 构建primary backup system可能会出现的issues or challenges？
		- Has the primary failed?
			- 在一个分布式系统中，很难区分是network partition还是machine failed，前者是说primary is still up但是部分计算机无法和primary进行通信，那么此时backup就会认为primary is dead，但是实际上一些client仍然可以与primary进行通信，这样也就会造成“two primary”的情形了。当network被heal了之后，那么原先的primary就能够正常继续工作了，此时一部分client与原先的primary进行通信，另外一部分client与新选出来的primary进行通信，于是整个系统就处于incorrect state当中
			- 所以，我们必须avoid this split brain syndrome at all costs
		- How to keep the primary and backup in sync ?
			- 当primary crash了，切换到backup时需要backup能够与原先primary的state保持一致，这样的切换就像是在一台电脑上无缝般切换一样流畅
			- 要求：
				- apply changes in the right order
				- avoid the non-determinism
					- 非确定性（Non-determinism）指的是在一个系统或算法中存在多种可能的结果或行为，无法准确预测或确定具体的结果。换句话说，非确定性描述了一个系统在某些情况下可以有多个可能的状态或演化路径
					- 这里是说我们要确保primary和backup中的changes和effects是完全identical的
			- fail over：the primary fails and the backup takes over
				- primary fail时可能正处在某个操作当中，比如正在发送一个数据packet给client作为response，但是也可能没有，我们需要搞清楚这个packet是否已经被发送了还是说只是处于发送阶段（如果被发送了要知道这个packet是否被client接收到了，如果接收到了，则不需要重发，否则发送了但未被接收或者只是处于发送准备，那么都需要进行packet的重发）
				- 如果所有的backup都fail了，那么当它们重新back up时，我们需要判断哪一个是具有最新的state
	- 两种deal with primary backup replication的方法分别是什么？
		- 第一种：
			- ![image.png](../assets/image_1688487464378_0.png)
			- primary在给客户端响应之前，要先执行相关的操作，同时需要把这个操作产生的状态变化同步给backup machine，这样才能保证两者的一致性
		- 第二种：
			- ![image.png](../assets/image_1688487897935_0.png)
			- primary在执行操作前，会把操作发送给backup，等待backup执行完这些操作后，发送acknowledgement给primary，然后primary再执行这些操作，最后才给客户端响应
			-
		- 第一种方法的缺点是什么呢？
			- 如果一个operation会产生很多state的话，那么state tranfer方式将变得代价很高，所以一般使用第二种方法，GFS中追加数据时没有send data而是发送operation就是这种方法
			-
		- 为什么在第二种方法中客户端不用发送数据呢？
			- 因为这些发送的operation是deterministic的，只要两台machine的当前state是一样的，那么应用这些操作，就一定会end up in the same state，因而也就保证了不同的machine具有最终相同的数据
		- 那么，对于所有程序的所有操作，要怎么判断它是deterministic的还是non-deterministic的？
			- 在replicated state machine中，是将所有的operation都变成deterministic
			-
		- 可以使用两者的混合方法吗？
			- 可以，你可以默认设置使用replicated state machine的方式来运行系统，但是如果primary或者backup fail了，那么就只剩下一台机器，然后需要创建一个新的replica，通常对于这个新的replica，实际上需要将  当前存在的replica的状态 或者  当前存在的replica的状态的副本 transfer到 它上面去
			- 整体上，这个混合方法中使用state transfer的operaton并不frequent，因为fail的情况并不总是出现，而replicated state machine方法的复制operations的行为比较频繁
			-
		- What level of operations to replicate ?
			- application-level (gfs file append, and write)
				- 这意味着应用必须参与其中，也就是应用能够知道这些操作的semantics是什么，能够知道一个操作或者事件实际上要去做什么，比如写实际上要去做什么
				- 如果在应用层面来使用replicated state machine这个方法，应用自身需要被进行修改，从而成为执行replicated state machine的一部分
			- machine level （processor level,  computer level）
				- 此时的state指的是x86 register和memory的状态，而operation指的是普通的电脑指令（instructions）
				- 在这个level进行replicate，可以使得replication完全透明：
					- 因为一台机器的应用是运行在OS之上的，应用底层只是运行了x86的指令，在replicated state machine方法中，backup只会重复这些x86指令的执行即可（add, divide, conditional branching and interrupting），所以应用无需进行修改
					- 那么这个replication是否很expensive呢?
						- 是的，因为machine level的replication意味着可能需要买很多台机器，也就是处理器会replicate若干次，这些处理器会ran exactly in lockstep （并行且高度一致），有很多的hardware machinery（硬件机械设备，可以指计算机硬件部件，也可以指其他行业中的物理设备或工具）可以做到这一点，但是在VMFT这篇论文中很cool的一点是可以使用virtual machine来替代hardware replication
						- pure hardware replication也有，比如在aeronautics中，通常是duplex  replicated或者triple replicated，且会有一个hardware voting scheme来使得processors保持同步以及检测failures
						- VMFT中并没有太focus fault tolerance的内容，通常是使用fertilization技术 [[存疑]]
						-
	- 关于VMFT这篇论文，你有哪些认识或者理解？
		- ![image.png](../assets/image_1688525744941_0.png)
			- 论文中的solution支持多核吗？为什么？
			  collapsed:: true
				- 论文中是single-core的，所以对多线程、多应用是无法在单核机器上并行运行的；但是之后版本的ft课程老师认为是可以multi-core support的，但是没有一篇详细的论文对此进行了描述，课程老师认为实际上可以使用state transfer approach来实现这一点
			- virtualization的好处是什么？
			  collapsed:: true
				- 使得replication是transparent的
				- 对于客户端来说，会认为服务器端只有一台机器
				-
			- VMFT这篇论文为什么有阅读价值？
			  collapsed:: true
				- 因为它非常详细地阐述了复制状态机，后续我们要学习的replication scheme都是属于复制状态机这种方法
			- 关于virtualization的overview？
			  collapsed:: true
				- virtual machine monitor / hypervisor
					- ![image.png](../assets/image_1688530813952_0.png)
						- 指的是在一块硬件的基础上使其成为电脑的end pieces（硬件系统的末端设备或者组件，通常直接与外部环境交互或者与其他系统进行连接）
						- 我们可以在x86 box（Hardware，图示中最底下的矩形）的上层运行一个virtual machine monitor，而在这个monitor上层通常是多个virtual machine，在论文中该vm monitor上层只有一个linux系统和相关的应用
						- vm monitor也被称作Hypervisor（虚拟机监控器），这里也就是论文中的vmft（vm with fault tolerance）
						  collapsed:: true
							- 是一种虚拟化技术，它允许在一台物理计算机上运行多个虚拟机操作系统。Hypervisor 可以将计算机的物理资源（如 CPU、内存、存储空间等）划分为多个虚拟机，每个虚拟机都可以独立运行不同的操作系统和应用程序。Hypervisor 可以提高计算机资源的利用率，同时也可以提供更好的安全性和可管理性。
						- 当最底层发生了Hardware interrupt，整个系统会怎么做？
							- 这个中断会先通过vm monitor，然后再由这个monitor将这个中断deliver给linux系统
							- 实际上，任何的external event，都会首先被hypervisor所捕获到，然后才会被vm捕获到
							- 如果有一个external interrupt，我们要怎么replicate this？
							  collapsed:: true
								- hypervisor实际上控制了interrupt的转发，所以replicate也与hypervisor有关。实际上，hypervisor是一个强有力的工具，能够使得指令变得deterministic
								- 如果有一个中断发生，不管是网络中断还是硬件中断（比如timer interrupt），vmft会做两件事：
								  collapsed:: true
									- 把这个中断给deliver给linux文件系统的application
									- 通过一个logging channel把这个中断发送给backup
									- 例子图示：
										- Client通过网络向Hardware发送一个packet，造成一个中断（下图中最底层的横线表示same network）
										  collapsed:: true
											- ![image.png](../assets/image_1688532422734_0.png)
										- linux系统接收到monitor传送过来的中断后，按照正常应对中断的方式进行处理，然后把想要发送的packet传送给monitor，linux系统会以为自己正在通过实际的网卡（network interface card）写入数据，但是实际上它是通过由monitor所仿真的（emulated）virtual network interface card来写入数据；当linux系统向virtual card写入一系列的指令时，它实际上正在通过monitor来进行写入；monitor通过对硬件编程（programming the real hardware）来代表os发送packet, 然后the real hardware将真正的响应返回给客户端
										  collapsed:: true
											- ![image.png](../assets/image_1688532840884_0.png)
										- 如果primary和backup初始时处于相同的state，如果它们同时接受同样的中断请求，monitor可以控制中断在同一时刻被delivered，且中断对应完全一样的指令；由于发送的中断需要通过logging channel才能传递到backup，所以中断在primary的monitor处会显buffer一段时间，直到中断也已经到达backup的monitor了
										- backup上linux返回的packet不会send到网络上
										  collapsed:: true
											- ![image.png](../assets/image_1688534687494_0.png)
										-
						- 还存在额外的component需要注意的吗？
							- 有，是在同一个网络上的Storage Server
							  collapsed:: true
								- ![image.png](../assets/image_1688534805884_0.png)
							- 它可以看作是primary和backup上两个virtual machine的hard disk：
							  collapsed:: true
								- 当一个应用试图给一个文件里写入数据，文件系统就会mount上linux系统（下图中green arrow表示了相关的控制流程）
								  collapsed:: true
									- ![image.png](../assets/image_1688562571800_0.png)
								- 有时候是客户端发起存储相关的communication，有时则是storage server主动发起存储相关的communication
							- storage server扮演了除了存储服务器以外的其他的额外的角色：arbitration server
							  collapsed:: true
								- 在storage server内部会有一个block用作flag，来arbitrate（make a formal judgment or make an official decision）当发生failure将成为primary的那台机器是哪一个
								- 主要原因是当primary和backup中某一个crash或者network fail时，无法区分是哪一种情况，但此时两者和storage server内部的这个flag block都是可以进行通信的；
								  collapsed:: true
									- 为什么这样做能够区分crash或者network fail呢?
										- 当crash时，其实只有一台仍旧alive的机器会进行 test and set操作，crash的那台无法与存储服务器通信，自然也没有机会成为primary
										- 而当network fail时，两台机器都会进行test and set 操作，两台机器中与存储服务器通信速度更快的将成为primary
										- 无论是哪种情况，最终得到的primary的选择项都是合理的
										-
								- 于是，primary和backup都是尝试往这个flag block里面写入1，谁先写入1谁就就是primary：假如backup比primary率先抢到了1，server就会给backup返回0；那么当primary读取这个flag block时就会发现它已经是1了，server就会返回给它1；这种操作被称作 test and set （先进行读取测试，然后依据测试结果进行相应的set），其中返回结果得到0的machine将会处理client发送的请求，而返回结果为1的machine将会terminate and done
							- arbitration server避免了split brain syndrome
						- vmft中的repair plan是什么？repair的意思是说当primary宕机了，现在就只剩下一台backup机器了，但是这台backup的机器也有可能进一步宕机，所以必须需要再有一台replica
						  collapsed:: true
							- vmft中的repair plan是手动的，就是 ：
								- 人通过monitor software会notice到这一点
								- 然后会主动创建一个新的replica，或者 基于第一台机器的镜像来创建：注意在这个复制或者说克隆期间，primary是不会处理来自客户端的请求的
								- 一旦second backup被创建成功，logging channel就会重新启动，如果我们follow the protocol, 这个flag就会被重设为0：这些操作是为了保证secondary backup和primary保持同步的状态
								  collapsed:: true
									- ![image.png](../assets/image_1688527000155_0.png)
								- 整个系统恢复对客户端的服务
						- 如果 logging channel或者the channel to the sever broke了，那我们就得不到响应了，所以怎么办呢？
						  collapsed:: true
							- sytem将直接stop，直到故障被repair，没有其他的补救措施
							- 且之后的所有客户端请求都不会再被接受处理，因为我们并不清楚当前的state是什么
						- 你认为disaster failures是什么？
						  collapsed:: true
							- 所有的网络电缆都断了
							- primary和backup同时都宕机了
							-
						- client什么时候会从storage server进行读取？
						  collapsed:: true
							- 不会，只有linux上的应用和vmft会与存储服务器进行交互，图示中的绿色箭头表示了这种流向：
								- ![image.png](../assets/image_1688565390164_0.png)
				-
			- primary / backup 的目标是两台机器工作的结果就像一台机器在运行，那么论文中这种replication sheme会存在哪些sources of divergence?
			  collapsed:: true
				- ![image.png](../assets/image_1688570772507_0.png)
				- 指令可能是non-deterministic的
					- 比如关于time的指令，如果两台机器的定时不同步，那么同一个指令的运行结果可能会是不同的
				- packets input or timer interrupt在不同机器的指令流中的位置可能不同（这里的位置可以理解为interrupt被deliver到指令流中的时机）
					- 当客户端发送packets时，或者产生了interrupt，必须要进行执行或者处理，这是由linux系统中对应的interrupt handler来具体进行处理
					- 比如primary中interrupt在指令1：inc 和 指令2：deck中发生，而同样的interrupt在backup中的指令2和指令3中发生了，这就是说中断不处于指令流中的same point，会导致指令流执行的最终结果可能不同
				- concurrency / multi-core 可能导致的non-determinism
					- 论文中的solution是disallow多核，只匀速进行uni-core的处理
					- 这是因为：
						- hypervisor控制了interrupt的处理，如果是uni-core就不会发生线程切换
						- 假设我们有multi-core，此时两个thread正在同一个processor的不同core上运行，两个thread争夺一个lock，其中一个会win，另一个会失败，如果我们要执行replicated state machine策略的话，在backup上的两个thread竞争lock的结果必须与primary是一致的，而这需要大量的machinery，这带来的额外的machinery和complexity是论文不想要或者无法解决的；所以，我们就直接通过uni-core来避免这种divergence的出现
						- uni-core导致的结果是：
							- primary上切换到某个thread A，backup也同步切换到A
					-
				-
			- 当interrupt comes in，primary和backup是怎么保证中断的同步的？
			  collapsed:: true
				- ![image.png](../assets/image_1688572740316_0.png)
					- VMFT判断应用或者linux系统要在哪个指令之后执行这个中断
					- 将【中断的前一个指令序号】+【中断指令本身】+【中断关联的数据】一并作为消息通过logging channel发送给backup，所以backup会lag behind one message
					-
			- 是不是只有interrupt等这些会造成potential divergence的指令才会需要通过logging channel来进行通信？
			  collapsed:: true
				- 是的，这里需要把operation和instruction两个概念区分清楚，linux系统执行program实际上底层是执行这些computer instructions，而在primary和backup上linux的所有computer instructions都是一样的，只有external interrupt是primary当前有而backup暂时还没有的，所以才需要通信来传递
				-
			- primary和backup对应的linux系统，对non-deterministic instruction是怎么进行处理的呢？
			  collapsed:: true
				- 首先，在linux系统通过image启动前，会先扫描找出linux binary中所有的non-determin instructions，比如 get the time of the day, 然后将它们转化成某种invalid instructions；
				- 然后，当linux实际去执行non-determin instructions时，它将控制权转交给hypervisor（hypervisor会把自动地识别到这条指令将cause a trap），hypervisor会对这个指令emulate(仿真），也就是知道这个指令将会产生的所有effects但只记录这些effects所产生的results，比如说执行指令“go into a0”，它会把a0的值记录下来并传递给backup
				- 最后，backup的vm monitor会使用同样的a0的值，将a0存入对应的需要更新数值的寄存器中，而不会重新执行指令
			- primary和backup执行的速度谁快谁慢呢?
			  collapsed:: true
				- 在几乎一样的硬件条件下，如果要执行的instructions完全没有中断的话，那么primary和backup的运行速度几乎是完全一样的；但是实际上总是不可避免地会有一些中断，比如定时中断，而处理中断时backup必然是lag behind primary one message 的，也就是primary会比backup要运行得可能慢一点，但是两者的运行速度不会差距太多，一方面是因为中断不会特别频繁，另一方面如果运行速度差太多会影响后面failure时backup取代primary提供服务时的性能
			- 关于failover的更多场景，能给出一些解读吗？
			  collapsed:: true
				- 场景1：
					- ![image.png](../assets/image_1688575324178_0.png)
					- 此时backup会先等待中断的消息完全接受，然后执行与primary相同的指令，但是并不响应回客户端；backup会通过arbitration server成为primary，客户端然后会超时重试，重新发送一个tcp packet，一切开始正常工作
				- 场景2：
					- ![image.png](../assets/image_1688575962260_0.png)
					- 场景描述：
						- 客户端发送请求让primary把变量从10增加到11，假如primary通过logging channel发送的消息never made it to the backup，primary在刚好把response返回给客户端前就fail了
					- 场景分析：
						- 当backup接管时，它不会意识到该自增操作，只会继续执行下去，比如继续自增1，它会返回给客户端11而不是正常的12
					- 场景解决：怎么避免？
						- 当primary通过logging channel给backup发送消息时，需要等待来自backup的acknowledgement，一旦primary接受到了这个acknowledgement，primary才会send output
						- backup并不需要实际上真地去执行这些命令，它只需要在它的logs进行记录，在成为primary之前catch up到这些信息就行
						- 这个被称作“Output Rule”：在primary发送输出前，它必须保证proceeding messages已经被backup所接收了
						-
			- 关于VMFT这篇论文中replication schema的性能，要如何进行评价？
			  collapsed:: true
				- 因为VMFT是在machine instruction的level上进行replication，所以不可避免地会损伤一些性能
				- 原论文中其性能有关的两张表如下：
					- ![image.png](../assets/image_1688608467177_0.png)
					- 从第一个表中可以得出什么结论？
						- FT就是有backup，non-FT就是没有backup，FT的性能会比non-Ft要差，因为会涉及backup的切换和同步等开销，但是表中的0.98、0.95等都只是略小于1，说明这种开销在VMFT中并不高，所以总体的performance很不错
					- 从第二个表中可以得出什么结论？
						- base指的是没有backup，FT就是有backup，Receive从940到604可以看到带宽解决有30%的下降。究其原因，primary从输入这里接收客户端发送的packet，但是它必须等到接收到了通过logging channel发送的来自backup的acknowledgement，才能最终给客户端一个response，所以有backup的结构不可能具有和单个机器一样的处理packet的速度
						- 但是，总体来说这个performance还是impressive的
						- 鉴于速度原因，通常人们使用replicated state machine方法不是像这篇论文中的instruction level，而是在application level，因为这将获得更高的性能，但需要application被modified，这一点我们已经在gfs系统中得见了
						-
						-
						-
			- 使用VMFT是否是一个design decision？
			  collapsed:: true
				- 是的，它使得这种replication变得transparent
				- 整个系统的设计目的就是：fault-tolerantly replicated
			- 在Output Rule下，客户端是否会两次看到同样的response呢？
			  collapsed:: true
				- 是，因为网络的fault-tolerant model总是assume：网络无论如何都可以duplicate message，而像tcp这样的协议在用来duplicate message方面已经可以说是完全设计好了, 无论应用使用什么样的replication或者duplication策略都是这样
				-
				-
				-
				-
			- 论文中如果出现了多个backup，那么对应的storage server除了arbitration还有其他的功能吗？
			  collapsed:: true
				- 不会出现这种情况，因为论文中的策略就只有一个backup
				-
			- 论文中说logging channel使用udp是为了传输的performance（延迟低），那么当primary通过这个channel发送packet但是没有收到acknowledgement时，是不是就会认为backup fail了，并且也不会重新再次传送（retransmit）?
			  collapsed:: true
				- No，比如说timer interrupt每隔开10毫秒就产生一次，如果primary发送的heartbeats返回，那么系统就会继续做一些timer interrupt，直到放弃
				- 这个heartbeats sort of来说是由timer发送的，但是由于每隔10毫秒就会发送一次interrupt message,所以会产生一些间接的副作用